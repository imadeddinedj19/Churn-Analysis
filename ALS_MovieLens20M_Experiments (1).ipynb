{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5798eb5f-54f1-4e66-97e1-f11e8d93c673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ALS on MovieLens 20M (Spark ML)\n",
    "\n",
    "This notebook:\n",
    "- loads the **MovieLens 20M** dataset from Databricks datasets\n",
    "- trains multiple **ALS** models with different hyperparameters\n",
    "- compares them using **RMSE**\n",
    "- visualizes how RMSE changes with parameters\n",
    "- adds **>=10 custom ratings** for a new user and shows **top‑20 recommendations**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6fc810-31d5-4ee6-beb8-75ae51cd7190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls('/databricks-datasets/cs110x/ml-20m/data-001/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673bc461-cdbc-4f1d-b734-a70d480c5e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths (Databricks datasets)\n",
    "MOVIES_PATH  = \"/databricks-datasets/cs110x/ml-20m/data-001/movies.csv\"\n",
    "RATINGS_PATH = \"/databricks-datasets/cs110x/ml-20m/data-001/ratings.csv\"\n",
    "\n",
    "display(dbutils.fs.ls(\"/databricks-datasets/cs110x/ml-20m/data-001/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74bee50-cd8f-4487-b505-b2fcde402dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, LongType\n",
    "\n",
    "movies_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True),\n",
    "])\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f482348-8262-4a54-af98-49e83b9a6602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_movies = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(movies_schema)\n",
    "    .load(MOVIES_PATH)\n",
    ")\n",
    "\n",
    "df_ratings = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(ratings_schema)\n",
    "    .load(RATINGS_PATH)\n",
    "    .select(\"userId\",\"movieId\",\"rating\")  # timestamp not needed for ALS here\n",
    ")\n",
    "\n",
    "print(\"movies:\", df_movies.count())\n",
    "print(\"ratings:\", df_ratings.count())\n",
    "display(df_movies.limit(5))\n",
    "display(df_ratings.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e11387-517d-400b-a8e7-2cf8fa5fbf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Quick sanity checks\n",
    "df_ratings.select(\"rating\").describe().show()\n",
    "df_ratings.select(F.countDistinct(\"userId\").alias(\"n_users\"),\n",
    "                  F.countDistinct(\"movieId\").alias(\"n_movies\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f03c32b-8c25-40e0-9fee-5ea7759dcb3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train / test split\n",
    "df_train, df_test = df_ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Cache for repeated fits (REMOVED due to serverless limitation)\n",
    "# df_train = df_train.cache()\n",
    "# df_test  = df_test.cache()\n",
    "\n",
    "print(\"train:\", df_train.count())\n",
    "print(\"test :\", df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597203d5-8ec1-431f-8776-db0a9ccc1d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "def train_eval(rank: int, regParam: float, maxIter: int):\n",
    "    als = ALS(\n",
    "        rank=rank,\n",
    "        regParam=regParam,\n",
    "        maxIter=maxIter,\n",
    "        userCol=\"userId\",\n",
    "        itemCol=\"movieId\",\n",
    "        ratingCol=\"rating\",\n",
    "        coldStartStrategy=\"drop\"   # drop NaN predictions at eval time\n",
    "    )\n",
    "    model = als.fit(df_train)\n",
    "    preds = model.transform(df_test)\n",
    "    rmse = evaluator.evaluate(preds)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3be4710-e56a-497a-aa16-5885b817228b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Hyperparameter search\n",
    "We try multiple values of:\n",
    "- `rank`\n",
    "- `regParam` (regularization)\n",
    "- `maxIter` (iterations)\n",
    "\n",
    "RMSE is lower = better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68443b78-07bb-4b71-9b9b-10bd643ebc46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameter grids (adjust if your cluster is small)\n",
    "ranks     = [10, 20, 30, 40]\n",
    "regParams = [0.01, 0.05, 0.1, 0.2]\n",
    "maxIters  = [5, 10, 15]\n",
    "\n",
    "results = []\n",
    "for r in ranks:\n",
    "    for reg in regParams:\n",
    "        for it in maxIters:\n",
    "            rmse = train_eval(rank=r, regParam=reg, maxIter=it)\n",
    "            results.append((r, reg, it, float(rmse)))\n",
    "            print(f\"rank={r:>2}  regParam={reg:<5}  maxIter={it:>2}  -> rmse={rmse}\")\n",
    "\n",
    "results_df = spark.createDataFrame(results, [\"rank\", \"regParam\", \"maxIter\", \"rmse\"])\n",
    "display(results_df.orderBy(\"rmse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13342b98-c577-4b30-9e1c-8d580da4d0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best model params (min RMSE)\n",
    "best_row = results_df.orderBy(\"rmse\").first()\n",
    "best_rank, best_reg, best_iter, best_rmse = best_row[\"rank\"], best_row[\"regParam\"], best_row[\"maxIter\"], best_row[\"rmse\"]\n",
    "print(\"BEST:\", best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1b6709-262e-4468-af34-28fd830604c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualizations\n",
    "We’ll plot RMSE as a function of the tested values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09fb336-f9a2-4fab-9728-e1a994386173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to pandas for plotting\n",
    "pdf = results_df.toPandas()\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0119599-7609-4faf-8150-894880f08238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) RMSE vs rank (one line per regParam) for a chosen maxIter (here: best_iter)\n",
    "fixed_iter = int(best_iter)\n",
    "sub = pdf[pdf[\"maxIter\"] == fixed_iter].copy()\n",
    "\n",
    "for reg in sorted(sub[\"regParam\"].unique()):\n",
    "    d = sub[sub[\"regParam\"] == reg].sort_values(\"rank\")\n",
    "    plt.plot(d[\"rank\"], d[\"rmse\"], marker=\"o\", label=f\"reg={reg}\")\n",
    "\n",
    "plt.xlabel(\"rank\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(f\"RMSE vs rank (maxIter={fixed_iter})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abff6f3-fc2d-49df-9e32-a88089607ca2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Heatmap: RMSE for rank x regParam at best_iter\n",
    "sub = pdf[pdf[\"maxIter\"] == fixed_iter].copy()\n",
    "pivot = sub.pivot(index=\"regParam\", columns=\"rank\", values=\"rmse\").sort_index()\n",
    "\n",
    "plt.imshow(pivot.values, aspect=\"auto\")\n",
    "plt.xticks(range(len(pivot.columns)), pivot.columns)\n",
    "plt.yticks(range(len(pivot.index)), pivot.index)\n",
    "plt.xlabel(\"rank\")\n",
    "plt.ylabel(\"regParam\")\n",
    "plt.title(f\"RMSE heatmap (maxIter={fixed_iter})\")\n",
    "plt.colorbar(label=\"RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbcaffd-7eb2-4995-b867-0ca05e99b198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) RMSE vs maxIter for best_rank + best_reg\n",
    "sub = pdf[(pdf[\"rank\"] == best_rank) & (pdf[\"regParam\"] == best_reg)].sort_values(\"maxIter\")\n",
    "plt.plot(sub[\"maxIter\"], sub[\"rmse\"], marker=\"o\")\n",
    "plt.xlabel(\"maxIter\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(f\"RMSE vs maxIter (rank={best_rank}, regParam={best_reg})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb0b344-3e79-4096-bdaf-521ee8b9ed0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train best ALS on full ratings + your custom ratings\n",
    "We add a new user with **at least 10 ratings** and ask for top‑20 recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11e6efb1-b9d6-4ce9-ba7f-4c53eb4e5370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Choose a new user id (one more than max existing)\n",
    "new_user_id = df_ratings.agg(F.max(\"userId\")).first()[0] + 1\n",
    "print(\"new_user_id:\", new_user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5954c6-e54e-4c24-89ad-50789950358f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pick at least 10 movies (MovieLens movieId is consistent across datasets)\n",
    "my_rated_movies = [\n",
    "    (new_user_id, 318, 5.0),   # Shawshank Redemption\n",
    "    (new_user_id, 858, 5.0),   # Godfather\n",
    "    (new_user_id, 296, 5.0),   # Pulp Fiction\n",
    "    (new_user_id, 260, 4.5),   # Star Wars: Episode IV\n",
    "    (new_user_id, 593, 4.0),   # Silence of the Lambs\n",
    "    (new_user_id, 527, 4.5),   # Schindler's List\n",
    "    (new_user_id, 50,  4.0),   # Usual Suspects\n",
    "    (new_user_id, 2571,4.0),   # Matrix\n",
    "    (new_user_id, 110, 4.0),   # Braveheart\n",
    "    (new_user_id, 1196,4.5),   # Star Wars: Episode V\n",
    "    (new_user_id, 1210,4.0),   # Star Wars: Episode VI\n",
    "]\n",
    "\n",
    "df_my_ratings = spark.createDataFrame(my_rated_movies, [\"userId\",\"movieId\",\"rating\"])\n",
    "\n",
    "# Show the \"my ratings\" table with titles\n",
    "df_my_ratings_named = (df_my_ratings\n",
    "    .join(df_movies, on=\"movieId\", how=\"left\")\n",
    "    .select(\"movieId\",\"title\",\"genres\",\"rating\")\n",
    "    .orderBy(F.desc(\"rating\"), \"title\")\n",
    ")\n",
    "display(df_my_ratings_named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb40ba55-6d96-4a10-bee0-aecee9963a1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the best ALS on ALL ratings + my ratings\n",
    "df_all = df_ratings.unionByName(df_my_ratings)\n",
    "\n",
    "als_best = ALS(\n",
    "    rank=int(best_rank),\n",
    "    regParam=float(best_reg),\n",
    "    maxIter=int(best_iter),\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "best_model = als_best.fit(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90d8f74-3aa7-4641-b784-558c5573bd2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ensure types match\n",
    "df_movies = df_movies.withColumn(\"movieId\", F.col(\"movieId\").cast(\"int\"))\n",
    "df_my_ratings = df_my_ratings.withColumn(\"movieId\", F.col(\"movieId\").cast(\"int\"))\n",
    "\n",
    "new_user_id_int = int(new_user_id)\n",
    "\n",
    "# Candidate set: all movies (for 1 user => ~27k rows, cheap)\n",
    "df_candidates = (\n",
    "    df_movies.select(\"movieId\").distinct()\n",
    "    .withColumn(\"userId\", F.lit(new_user_id_int))\n",
    ")\n",
    "\n",
    "# Predict ratings for all candidate movies\n",
    "df_pred = best_model.transform(df_candidates)\n",
    "\n",
    "# Remove already-rated movies + null/NaN predictions\n",
    "df_pred_clean = (\n",
    "    df_pred\n",
    "    .join(df_my_ratings.select(\"movieId\").withColumn(\"already_rated\", F.lit(1)),\n",
    "          on=\"movieId\", how=\"left\")\n",
    "    .filter(F.col(\"already_rated\").isNull())\n",
    "    .filter(F.col(\"prediction\").isNotNull())\n",
    "    .filter(~F.isnan(\"prediction\"))\n",
    ")\n",
    "\n",
    "# Attach movie names and pick Top-20\n",
    "recs_named = (\n",
    "    df_pred_clean\n",
    "    .join(df_movies, on=\"movieId\", how=\"left\")\n",
    "    .select(\"movieId\", \"title\", \"genres\", F.round(\"prediction\", 3).alias(\"pred_rating\"))\n",
    "    .orderBy(F.desc(\"pred_rating\"))\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "display(recs_named)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ee4000-da21-4f58-a50d-d009a94daaa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notes\n",
    "- If runtime is too long, reduce the grids (`ranks`, `regParams`, `maxIters`) or sample ratings before training.\n",
    "- `coldStartStrategy='drop'` avoids NaN predictions during evaluation.\n",
    "- RMSE is computed on the held‑out test set.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6911556489207643,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ALS_MovieLens20M_Experiments (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
